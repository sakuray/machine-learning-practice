'''
    决策树：根据输入不断的选择树的路径，直到获得结果
    优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
    缺点：可能会产生过度匹配问题
    适用数据类型：数值型和标称型

    工作原理：选出重要、有决定性的特征来构建决策树，输入的数据根据树的判断条件，一步步走向不同的分支，获得最后结论

    信息增益：使用信息论度量信息，可以计算每个特征值划分数据获得的增益，增益越高的特征越好

    （香农熵）
    熵是信息的期望值：一个待分类的信息可能划分在多个分类中，该信息定义为：
        l(xi) = -log2p(xi)      p(xi)是选择该分类的概率
    H = -∑p(xi)log2p(xi)

    特征重要性判断方法：
        一个有m个特征的n条数据的数据集，其最终被划分成k个分类，
        首先计算整个数据集的信息熵：即k个分类出现的预期*其信息定义
        再计算每个特征子数据集，该特征的可能值有t个，计算每个可能值的熵：
        即计算特征A值为X的信息熵，统计整个特征A所有可能值的信息熵和
        不是简单的加起来，而是要其信息熵*该特征值特定值在整个数据集中出现的概率，再累加起来
        最终整个数据集的信息熵-算出的特征A的信息熵，得到的值最大的那个特征就是最重要的特征了
    简单理解就是：
        期望=结果*概率
        数据集的信息熵=信息定义*概率
        特征的信息熵=该特征值特定解子集合的信息熵*特征值特定解的概率（所有解之和）
        信息增益 = 数据集的信息熵 - 特征的信息熵
'''